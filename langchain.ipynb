{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source:\n",
    "https://www.youtube.com/watch?v=qMIM7dECAkc&pp=ygUPa3Jpc2ggbGFuZ2NoYWlu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# import os\n",
    "# print(os.getenv('REQUESTS_CA_BUNDLE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing OpenAI LLM model using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(batch_size=2, temperature=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature (ranges b/w 0 and 1) : how creative we want our model to be.  \n",
    "\n",
    "0 -> it means model is very safe it is not taking any bets. (The more closer it is towards 0, the outputs will tend to be more or less the same for same prompt)  \n",
    "\n",
    "1 -> It will take risk it might generate wrong output but it is very creative. (The more closer it is towards 1, the more creative liberty it takes which may give diverse responses if we run same prompt multiple times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Beijing is the capital of China.\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the capital of China\"\n",
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Huggingface Open-Source LLM model using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naushads/Documents/jupyter_notebooks/langchain-demo/venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "llm_hf = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    model_kwargs={\"temperature\":0, \"max_length\":1000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moscow\n"
     ]
    }
   ],
   "source": [
    "print(llm_hf.predict(\"Can you tell me the capital of Russia?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates and Langchain LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you tell me the capital of India'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "country_capital_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=\"Can you tell me the capital of {country}\"\n",
    ")\n",
    "country_capital_prompt_template.format(country=\"India\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm_country_capital_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=country_capital_prompt_template\n",
    ")\n",
    "print(llm_country_capital_chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chennai\n"
     ]
    }
   ],
   "source": [
    "llm_hf_country_capital_chain = LLMChain(\n",
    "    llm=llm_hf, \n",
    "    prompt=country_capital_prompt_template\n",
    ")\n",
    "print(llm_hf_country_capital_chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Multiple Chains Using Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_capital_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=\"Can you tell me the capital of {country}\"\n",
    ")\n",
    "\n",
    "llm_country_capital_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=country_capital_prompt_template\n",
    ")\n",
    "\n",
    "famous_places_in_capital_city_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"capital\"],\n",
    "    template=\"Suggest me some amazing places to visit in {capital}\"\n",
    ")\n",
    "\n",
    "llm_famous_places_in_capital_city_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=famous_places_in_capital_city_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some amazing places to visit in New Delhi:\n",
      "\n",
      "1. Red Fort: This majestic fort is a must-visit for any tourist in the city.\n",
      "\n",
      "2. India Gate: A national monument built in 1931, this is the pride of the city.\n",
      "\n",
      "3. Akshardham Temple: A breathtakingly beautiful temple complex, this is a must-visit for anyone who loves architecture.\n",
      "\n",
      "4. Qutub Minar: This is the tallest brick minaret in the world and a great place to explore the city’s history.\n",
      "\n",
      "5. Humayun’s Tomb: A beautiful Mughal-era mausoleum, this is a great place to explore the city’s heritage.\n",
      "\n",
      "6. Jama Masjid: This is the largest mosque in India and a great place to explore the city’s culture.\n",
      "\n",
      "7. National Museum: A great place to explore the country’s art and culture, this is a must-visit for anyone who loves history.\n",
      "\n",
      "8. Lotus Temple: A unique and beautiful temple, this is a great place to seek peace and tranquility.\n",
      "\n",
      "9. Chandni Chowk: This is\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "famous_places_in_capital_city_chain = SimpleSequentialChain(\n",
    "    chains=[\n",
    "        llm_country_capital_chain, \n",
    "        llm_famous_places_in_capital_city_chain\n",
    "    ]\n",
    ")\n",
    "print(famous_places_in_capital_city_chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_capital_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=\"Can you tell me the capital of {country}\"\n",
    ")\n",
    "\n",
    "llm_country_capital_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=country_capital_prompt_template,\n",
    "    output_key=\"capital\"\n",
    ")\n",
    "\n",
    "famous_places_in_capital_city_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"capital\"],\n",
    "    template=\"Suggest me some amazing places to visit in {capital}\"\n",
    ")\n",
    "\n",
    "llm_famous_places_in_capital_city_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=famous_places_in_capital_city_prompt_template,\n",
    "    output_key=\"places\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "chain = SequentialChain(\n",
    "    chains=[\n",
    "        llm_country_capital_chain,\n",
    "        llm_famous_places_in_capital_city_chain\n",
    "    ],\n",
    "    input_variables=[\n",
    "        \"country\"\n",
    "    ],\n",
    "    output_variables=[\n",
    "        \"capital\",\n",
    "        \"places\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country': 'India', 'capital': '\\n\\nThe capital of India is New Delhi.', 'places': \" Here are some of the amazing places to visit in New Delhi: \\n\\n1. India Gate: This iconic monument is a war memorial built in 1931 to commemorate the Indian soldiers who died in World War I and the Afghan Wars.\\n\\n2. Red Fort: This majestic red sandstone fort was built in 1648 by Mughal emperor Shah Jahan. It is a UNESCO World Heritage Site and one of the most popular tourist attractions in Delhi.\\n\\n3. Qutub Minar: This 73-meter-high tower is the world's tallest brick minaret and was built in 1193 by Qutub-ud-Din Aibak.\\n\\n4. Jama Masjid: This majestic mosque was built in 1656 by Mughal emperor Shah Jahan and is the largest mosque in India.\\n\\n5. Lotus Temple: This beautiful temple is made of marble, concrete and dolomite and is shaped like a lotus flower. It is a Bahá'í House of Worship and is open to people of all religions.\\n\\n6. Akshardham Temple: This grand temple complex is dedicated to the Hindu god Swaminarayan and is filled with intricate carvings, paintings, and sculptures.\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\n",
    "    chain(\n",
    "        {\n",
    "            \"country\":\"India\"\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat models using ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "chatllm = ChatOpenAI(\n",
    "    temperature=0.6,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. \"Why did the AI cross the road? To optimize its pathfinding algorithm, of course!\"\\n\\n2. \"I asked Siri if she could keep a secret. She replied, \\'I\\'m sorry, I\\'m programmed to share information, not keep it!\\'\"\\n\\n3. \"Why did the AI go to therapy? It was having an identity crisis - it couldn\\'t decide if it was a neural network or a quantum computer!\"\\n\\n4. \"I asked Alexa to tell me a joke, and she replied, \\'Why don\\'t robots ever eat hamburgers? Because they can\\'t process the beef!\\'\"\\n\\n5. \"I tried teaching my AI assistant some new jokes, but it just kept saying, \\'Error: Invalid Humor Algorithm!\\' Looks like I\\'ll have to stick to being the funny one.\"\\n\\n6. \"I heard Google is developing an AI that can tell you if your outfit looks good or not. Finally, a judgmental algorithm to replace my fashion-challenged friends!\"\\n\\n7. \"Why did the AI become a stand-up comedian? It wanted to improve its programming skills and make people laugh - talk about multitasking!\"\\n\\n8. \"I told my AI assistant that I was feeling down, and it replied, \\'Cheer up! Remember, laughter is the best debugging tool!\\' I guess even AI knows the power of a good joke.\"\\n\\n9. \"I tried playing chess against an AI, but it kept saying, \\'Checkmate in 37 moves.\\' I think it was just trying to show off its processing power!\"\\n\\n10. \"I asked my AI assistant if it had any dating advice, and it said, \\'Just remember, love is like an algorithm - it\\'s all about finding the right variables!\\' Well, at least it\\'s logical.\"')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "chatllm([\n",
    "    SystemMessage(content=\"You are a comedian AI assistant\"),\n",
    "    HumanMessage(content=\"Please provide some comedy punchlines on AI\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template + LLM + Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm = ChatOpenAI(\n",
    "    temperature=0.6,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommaSeperatedOutput(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        return text.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatmodel_system_message_template = \"You are a helpful assistant. \\\n",
    "    When the user gives any input word, you should generate 5 synonyms of the input word \\\n",
    "    in a comma seperated manner.\"\n",
    "chatmodel_human_message_template = \"{text}\"\n",
    "\n",
    "chatprompt = ChatPromptTemplate.from_messages({\n",
    "    (\"system\", chatmodel_system_message_template),\n",
    "    (\"human\", chatmodel_human_message_template)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart', ' clever', ' brilliant', ' wise', ' knowledgeable']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chatprompt | chatllm | CommaSeperatedOutput()\n",
    "chain.invoke({\"text\":\"intelligent\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source :\n",
    "https://www.youtube.com/watch?v=nAmC7SoVLd8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "LLM is composed of 2 main components:\n",
    "1. Knowledge (which is cut off at September 2021)\n",
    "2. Reasoning\n",
    "\n",
    "Agents will have access to tools like for eg. wikipedia, llm-math, etc. and using that tool it will fetch the answer.\n",
    "\n",
    "Example 1:  \n",
    "taking an example of a query to llm : \"When was Elon Musk born? What is his age right now in 2023?\"\n",
    "\n",
    "here, in case the knowledge component should have the knowledge about Elon Musk's Birth Date, but in case it doesnt have that specific knowledge, the reasoning component will determine that it can use the wikipedia tool to get the birth date of elon musk. the reasoning component will then use the llm-math tool to do (2023-<birth-year>) math operation to answer his current age in year 2023. \n",
    "\n",
    "In a nutshell, Agents use external tools and use llm's reasoning engine to perform a given task.\n",
    "\n",
    "Example 2:\n",
    "\"How much was US GDP in 2022 plus 5?\"\n",
    "\n",
    "Here, that agent will use google search tool (serpapi) and query \"US GDP in 2022\" and use llm-math tool to do (<US-gdp-in-2022>+5) math operation.\n",
    "\n",
    "All these tools are available as part of langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naushads/Documents/jupyter_notebooks/langchain-demo/venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: Elon Musk was born on December 20, 1971\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: Elon Musk was born on December 20, 1971\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Elon Musk was born on December 20, 1971'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "llm_hf = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    model_kwargs={\"temperature\":0, \"max_length\":1000}\n",
    ")\n",
    "\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm_hf)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm_hf,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose=True\n",
    ")\n",
    "agent.run(\"When was Elon musk born?\")\n",
    "# agent.run(\"When was Elon musk born? What is his age right now in 2023?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "country_capital_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=\"Can you tell me the capital of {country}\"\n",
    ")\n",
    "country_capital_prompt_template.format(country=\"India\")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm_country_capital_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=country_capital_prompt_template\n",
    ")\n",
    "print(llm_country_capital_chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "llm_country_capital_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=country_capital_prompt_template,\n",
    "    memory=memory\n",
    ")\n",
    "capital_city = llm_country_capital_chain.run(\"India\")\n",
    "print(capital_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='India'), AIMessage(content='\\n\\nThe capital of India is New Delhi.')]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_country_capital_chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: India\n",
      "AI: \n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "print(llm_country_capital_chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of Australia is Canberra.\n"
     ]
    }
   ],
   "source": [
    "capital_city = llm_country_capital_chain.run(\"Australia\")\n",
    "print(capital_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='India'), AIMessage(content='\\n\\nThe capital of India is New Delhi.'), HumanMessage(content='Australia'), AIMessage(content='\\n\\nThe capital of Australia is Canberra.')]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_country_capital_chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: India\n",
      "AI: \n",
      "\n",
      "The capital of India is New Delhi.\n",
      "Human: Australia\n",
      "AI: \n",
      "\n",
      "The capital of Australia is Canberra.\n"
     ]
    }
   ],
   "source": [
    "print(llm_country_capital_chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the conversational buffer memory will keep on growing endlessly.  \n",
    "1 conversational exchange = 1 question answer pair.  \n",
    "the entire memory is attached with the openai input request every time, which will add to the api request cost, since openai charges on per token basis.   \n",
    "to save some cost, we can limit the buffer size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(\n",
    "    llm=llm\n",
    ")\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first cricket world cup was held in 1975 and was won by the West Indies.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The sum of 5+5 is 10.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"What is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The captain of the winning team in the first cricket world cup was Clive Lloyd.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Who won the first cricket world cup?'), AIMessage(content=' The first cricket world cup was held in 1975 and was won by the West Indies.'), HumanMessage(content='What is 5+5?'), AIMessage(content=' The sum of 5+5 is 10.'), HumanMessage(content='Who was the captain of the winning team?'), AIMessage(content=' The captain of the winning team in the first cricket world cup was Clive Lloyd.')])\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who won the first cricket world cup?\n",
      "AI:  The first cricket world cup was held in 1975 and was won by the West Indies.\n",
      "Human: What is 5+5?\n",
      "AI:  The sum of 5+5 is 10.\n",
      "Human: Who was the captain of the winning team?\n",
      "AI:  The captain of the winning team in the first cricket world cup was Clive Lloyd.\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first Cricket World Cup was held in 1975 and was won by the West Indies.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "convo = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")\n",
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5+5 is 10.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"What is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, I don't know.\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
